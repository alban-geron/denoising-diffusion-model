{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BTW4tPbX1zb-",
        "outputId": "b2289d9c-c92e-4ac9-e3c3-5dbf481b70ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entraînement sur : cuda\n",
            "Dataset sera dans : c:\\Users\\alban\\Documents\\Cursor code\\denoising-diffusion-model\\dataset\n",
            "Samples seront dans : c:\\Users\\alban\\Documents\\Cursor code\\denoising-diffusion-model\\model code\\GANs\\samples\n",
            "Checkpoint sera : c:\\Users\\alban\\Documents\\Cursor code\\denoising-diffusion-model\\model code\\GANs\\wgan_mnist_ckpt.pth\n",
            "Aucun checkpoint trouvé. Démarrage de zéro.\n",
            "Début de l'entraînement...\n",
            "Epoch [0/20] Batch 0/938 Loss D: -3.6450, Loss G: 0.9150\n",
            "Epoch [0/20] Batch 400/938 Loss D: -21.4977, Loss G: 19.3148\n",
            "Epoch [0/20] Batch 800/938 Loss D: -21.1267, Loss G: 20.0049\n",
            "Epoch [1/20] Batch 0/938 Loss D: -21.4401, Loss G: 21.1709\n",
            "Epoch [1/20] Batch 400/938 Loss D: -22.2393, Loss G: 22.2292\n",
            "Epoch [1/20] Batch 800/938 Loss D: -21.7653, Loss G: 23.4041\n",
            "Epoch [2/20] Batch 0/938 Loss D: -21.8310, Loss G: 24.3075\n",
            "Epoch [2/20] Batch 400/938 Loss D: -5.6216, Loss G: 18.8907\n",
            "Epoch [2/20] Batch 800/938 Loss D: -5.3838, Loss G: 19.6463\n",
            "Epoch [3/20] Batch 0/938 Loss D: -5.0966, Loss G: 19.3377\n",
            "Epoch [3/20] Batch 400/938 Loss D: -5.7802, Loss G: 18.8934\n",
            "Epoch [3/20] Batch 800/938 Loss D: -5.5510, Loss G: 18.7402\n",
            "Epoch [4/20] Batch 0/938 Loss D: -5.0916, Loss G: 18.7830\n",
            "Epoch [4/20] Batch 400/938 Loss D: -4.5787, Loss G: 19.6914\n",
            "Epoch [4/20] Batch 800/938 Loss D: -4.7116, Loss G: 18.2877\n",
            "--> Sauvegarde Epoch 5\n",
            "    Checkpoint : wgan_mnist_ckpt.pth\n",
            "    Image : samples\\epoch_5.png\n",
            "Epoch [5/20] Batch 0/938 Loss D: -4.4063, Loss G: 17.6672\n",
            "Epoch [5/20] Batch 400/938 Loss D: -3.9077, Loss G: 18.0497\n",
            "Epoch [5/20] Batch 800/938 Loss D: -4.6087, Loss G: 18.4467\n",
            "Epoch [6/20] Batch 0/938 Loss D: -5.1070, Loss G: 18.4455\n",
            "Epoch [6/20] Batch 400/938 Loss D: -5.4115, Loss G: 18.0524\n",
            "Epoch [6/20] Batch 800/938 Loss D: -4.6055, Loss G: 17.8455\n",
            "Epoch [7/20] Batch 0/938 Loss D: -3.8952, Loss G: 16.7210\n",
            "Epoch [7/20] Batch 400/938 Loss D: -3.7596, Loss G: 15.9721\n",
            "Epoch [7/20] Batch 800/938 Loss D: -4.3360, Loss G: 16.6021\n",
            "Epoch [8/20] Batch 0/938 Loss D: -4.2545, Loss G: 16.8829\n",
            "Epoch [8/20] Batch 400/938 Loss D: -3.7694, Loss G: 15.6121\n",
            "Epoch [8/20] Batch 800/938 Loss D: -4.3817, Loss G: 16.7531\n",
            "Epoch [9/20] Batch 0/938 Loss D: -3.4588, Loss G: 14.8884\n",
            "Epoch [9/20] Batch 400/938 Loss D: -3.8593, Loss G: 15.7087\n",
            "Epoch [9/20] Batch 800/938 Loss D: -4.5190, Loss G: 15.6055\n",
            "--> Sauvegarde Epoch 10\n",
            "    Checkpoint : wgan_mnist_ckpt.pth\n",
            "    Image : samples\\epoch_10.png\n",
            "Epoch [10/20] Batch 0/938 Loss D: -3.6033, Loss G: 15.3637\n",
            "Epoch [10/20] Batch 400/938 Loss D: -4.2940, Loss G: 16.1552\n",
            "Epoch [10/20] Batch 800/938 Loss D: -4.4370, Loss G: 16.8811\n",
            "Epoch [11/20] Batch 0/938 Loss D: -3.6170, Loss G: 15.5086\n",
            "Epoch [11/20] Batch 400/938 Loss D: -4.1583, Loss G: 17.0171\n",
            "Epoch [11/20] Batch 800/938 Loss D: -3.8215, Loss G: 16.1889\n",
            "Epoch [12/20] Batch 0/938 Loss D: -3.9104, Loss G: 16.7922\n",
            "Epoch [12/20] Batch 400/938 Loss D: -4.0501, Loss G: 16.3581\n",
            "Epoch [12/20] Batch 800/938 Loss D: -4.5004, Loss G: 15.5608\n",
            "Epoch [13/20] Batch 0/938 Loss D: -3.6813, Loss G: 17.2595\n",
            "Epoch [13/20] Batch 400/938 Loss D: -3.7265, Loss G: 16.7295\n",
            "Epoch [13/20] Batch 800/938 Loss D: -3.6220, Loss G: 17.3229\n",
            "Epoch [14/20] Batch 0/938 Loss D: -3.5701, Loss G: 16.3763\n",
            "Epoch [14/20] Batch 400/938 Loss D: -2.8058, Loss G: 16.3253\n",
            "Epoch [14/20] Batch 800/938 Loss D: -3.5664, Loss G: 16.7252\n",
            "--> Sauvegarde Epoch 15\n",
            "    Checkpoint : wgan_mnist_ckpt.pth\n",
            "    Image : samples\\epoch_15.png\n",
            "Epoch [15/20] Batch 0/938 Loss D: -4.2333, Loss G: 15.6501\n",
            "Epoch [15/20] Batch 400/938 Loss D: -3.9115, Loss G: 16.2918\n",
            "Epoch [15/20] Batch 800/938 Loss D: -3.8522, Loss G: 16.5944\n",
            "Epoch [16/20] Batch 0/938 Loss D: -3.0081, Loss G: 15.1009\n",
            "Epoch [16/20] Batch 400/938 Loss D: -4.0837, Loss G: 15.6307\n",
            "Epoch [16/20] Batch 800/938 Loss D: -3.2776, Loss G: 14.9124\n",
            "Epoch [17/20] Batch 0/938 Loss D: -2.8541, Loss G: 15.0741\n",
            "Epoch [17/20] Batch 400/938 Loss D: -2.6920, Loss G: 14.4312\n",
            "Epoch [17/20] Batch 800/938 Loss D: -3.4083, Loss G: 13.8335\n",
            "Epoch [18/20] Batch 0/938 Loss D: -3.5927, Loss G: 13.9245\n",
            "Epoch [18/20] Batch 400/938 Loss D: -4.2255, Loss G: 16.0508\n",
            "Epoch [18/20] Batch 800/938 Loss D: -3.4434, Loss G: 15.4447\n",
            "Epoch [19/20] Batch 0/938 Loss D: -2.8277, Loss G: 16.0853\n",
            "Epoch [19/20] Batch 400/938 Loss D: -3.3301, Loss G: 15.2725\n",
            "Epoch [19/20] Batch 800/938 Loss D: -3.4770, Loss G: 13.4377\n",
            "--> Sauvegarde Epoch 20\n",
            "    Checkpoint : wgan_mnist_ckpt.pth\n",
            "    Image : samples\\epoch_20.png\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION & CHEMINS\n",
        "# ==========================================\n",
        "class Config:\n",
        "    # Matériel\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Hyperparamètres WGAN-GP\n",
        "    LR = 1e-4\n",
        "    BATCH_SIZE = 64\n",
        "    IMAGE_SIZE = 28\n",
        "    CHANNELS = 1\n",
        "    Z_DIM = 100\n",
        "    NUM_EPOCHS = 20\n",
        "    FEATURES_DIM = 64\n",
        "    CRITIC_ITERATIONS = 5\n",
        "    LAMBDA_GP = 10\n",
        "    \n",
        "    # --- GESTION DES CHEMINS ---\n",
        "    # On suppose que le script tourne dans \"model code/GANs/\"\n",
        "    \n",
        "    # Chemin vers: denoising-diffusion-model/dataset\n",
        "    # Torchvision ajoutera automatiquement le sous-dossier /MNIST\n",
        "    DATA_ROOT = os.path.join(\"..\", \"..\", \"dataset\") \n",
        "    \n",
        "    # Chemin vers: denoising-diffusion-model/model code/GANs/samples\n",
        "    IMG_DIR = \"samples\"\n",
        "    \n",
        "    # Nom du fichier checkpoint\n",
        "    CKPT_NAME = \"wgan_mnist_ckpt.pth\"\n",
        "    \n",
        "    # Fréquence de sauvegarde\n",
        "    SAVE_EVERY = 5\n",
        "\n",
        "conf = Config()\n",
        "print(f\"Entraînement sur : {conf.DEVICE}\")\n",
        "\n",
        "# Création du dossier pour les samples s'il n'existe pas\n",
        "os.makedirs(conf.IMG_DIR, exist_ok=True)\n",
        "\n",
        "# Chemin complet du fichier .pth (dans le dossier courant du notebook)\n",
        "ckpt_path_full = conf.CKPT_NAME\n",
        "\n",
        "print(f\"Dataset sera dans : {os.path.abspath(conf.DATA_ROOT)}\")\n",
        "print(f\"Samples seront dans : {os.path.abspath(conf.IMG_DIR)}\")\n",
        "print(f\"Checkpoint sera : {os.path.abspath(ckpt_path_full)}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. ARCHITECTURES (MNIST 28x28)\n",
        "# ==========================================\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Critic, self).__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            # Input: N x 1 x 28 x 28\n",
        "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1), # -> 14x14\n",
        "            nn.LeakyReLU(0.2),\n",
        "            \n",
        "            # 14x14 -> 7x7\n",
        "            nn.Conv2d(features_d, features_d * 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(features_d * 2, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            \n",
        "            # 7x7 -> 3x3\n",
        "            nn.Conv2d(features_d * 2, features_d * 4, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(features_d * 4, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            \n",
        "            # 3x3 -> 1x1\n",
        "            nn.Conv2d(features_d * 4, 1, kernel_size=3, stride=1, padding=0),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            # Input: Z (N x 100 x 1 x 1) -> 7x7\n",
        "            nn.ConvTranspose2d(z_dim, features_g * 4, kernel_size=7, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(features_g * 4),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 7x7 -> 14x14\n",
        "            nn.ConvTranspose2d(features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(features_g * 2),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # 14x14 -> 28x28\n",
        "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh(), # Sortie entre [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "# ==========================================\n",
        "# 3. UTILS : GRADIENT PENALTY\n",
        "# ==========================================\n",
        "def gradient_penalty(critic, real, fake, device):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    epsilon = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated = real * epsilon + fake * (1 - epsilon)\n",
        "    \n",
        "    # Indispensable pour calculer le gradient par rapport à l'entrée\n",
        "    interpolated.requires_grad_(True)\n",
        "\n",
        "    mixed_scores = critic(interpolated)\n",
        "    \n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    \n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    return torch.mean((gradient_norm - 1) ** 2)\n",
        "\n",
        "# ==========================================\n",
        "# 4. PRÉPARATION DES DONNÉES\n",
        "# ==========================================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(conf.IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])\n",
        "\n",
        "# Le dataset sera téléchargé dans ../../dataset/MNIST\n",
        "dataset = torchvision.datasets.MNIST(\n",
        "    root=conf.DATA_ROOT, \n",
        "    train=True, \n",
        "    transform=transform, \n",
        "    download=True\n",
        ")\n",
        "loader = DataLoader(dataset, batch_size=conf.BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Initialisation des modèles\n",
        "gen = Generator(conf.Z_DIM, conf.CHANNELS, conf.FEATURES_DIM).to(conf.DEVICE)\n",
        "critic = Critic(conf.CHANNELS, conf.FEATURES_DIM).to(conf.DEVICE)\n",
        "\n",
        "# Optimiseurs\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=conf.LR, betas=(0.0, 0.9))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=conf.LR, betas=(0.0, 0.9))\n",
        "\n",
        "# Bruit fixe pour visualiser la progression\n",
        "fixed_noise = torch.randn(64, conf.Z_DIM, 1, 1).to(conf.DEVICE)\n",
        "\n",
        "# ==========================================\n",
        "# 5. REPRISE DE CHECKPOINT (Load)\n",
        "# ==========================================\n",
        "start_epoch = 0\n",
        "if os.path.exists(ckpt_path_full):\n",
        "    print(f\"Chargement du checkpoint : {ckpt_path_full}\")\n",
        "    ckpt = torch.load(ckpt_path_full, map_location=conf.DEVICE)\n",
        "    gen.load_state_dict(ckpt[\"gen_state\"])\n",
        "    critic.load_state_dict(ckpt[\"critic_state\"])\n",
        "    opt_gen.load_state_dict(ckpt[\"opt_gen_state\"])\n",
        "    opt_critic.load_state_dict(ckpt[\"opt_critic_state\"])\n",
        "    start_epoch = ckpt[\"epoch\"] + 1\n",
        "else:\n",
        "    print(\"Aucun checkpoint trouvé. Démarrage de zéro.\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. BOUCLE D'ENTRAÎNEMENT\n",
        "# ==========================================\n",
        "print(\"Début de l'entraînement...\")\n",
        "\n",
        "for epoch in range(start_epoch, conf.NUM_EPOCHS):\n",
        "    gen.train()\n",
        "    critic.train()\n",
        "    \n",
        "    for batch_idx, (real, _) in enumerate(loader):\n",
        "        real = real.to(conf.DEVICE)\n",
        "        cur_batch_size = real.shape[0]\n",
        "\n",
        "        # ---------------------\n",
        "        # 1. Train Critique\n",
        "        # ---------------------\n",
        "        for _ in range(conf.CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(cur_batch_size, conf.Z_DIM, 1, 1).to(conf.DEVICE)\n",
        "            fake = gen(noise)\n",
        "            \n",
        "            critic_real = critic(real).reshape(-1)\n",
        "            critic_fake = critic(fake).reshape(-1)\n",
        "            \n",
        "            gp = gradient_penalty(critic, real, fake, conf.DEVICE)\n",
        "            \n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + conf.LAMBDA_GP * gp\n",
        "            )\n",
        "            \n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "        # ---------------------\n",
        "        # 2. Train Générateur\n",
        "        # ---------------------\n",
        "        gen_fake = critic(fake).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        \n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Log\n",
        "        if batch_idx % 400 == 0:\n",
        "            print(f\"Epoch [{epoch}/{conf.NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \"\n",
        "                  f\"Loss D: {loss_critic:.4f}, Loss G: {loss_gen:.4f}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # SAUVEGARDE PÉRIODIQUE (PTH + IMAGES)\n",
        "    # ==========================================\n",
        "    if (epoch + 1) % conf.SAVE_EVERY == 0 or (epoch + 1) == conf.NUM_EPOCHS:\n",
        "        print(f\"--> Sauvegarde Epoch {epoch+1}\")\n",
        "        \n",
        "        # A. Sauvegarde du modèle complet dans GANs/\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"gen_state\": gen.state_dict(),\n",
        "            \"critic_state\": critic.state_dict(),\n",
        "            \"opt_gen_state\": opt_gen.state_dict(),\n",
        "            \"opt_critic_state\": opt_critic.state_dict(),\n",
        "        }, ckpt_path_full)\n",
        "        \n",
        "        # B. Génération et sauvegarde des images dans GANs/samples/\n",
        "        gen.eval()\n",
        "        with torch.no_grad():\n",
        "            fake_img = gen(fixed_noise)\n",
        "            # Dénormalisation [-1, 1] -> [0, 1]\n",
        "            fake_img = (fake_img * 0.5) + 0.5\n",
        "            \n",
        "            save_name = f\"epoch_{epoch+1}.png\"\n",
        "            save_path = os.path.join(conf.IMG_DIR, save_name)\n",
        "            \n",
        "            save_image(fake_img, save_path, nrow=8)\n",
        "            print(f\"    Checkpoint : {ckpt_path_full}\")\n",
        "            print(f\"    Image : {save_path}\")\n",
        "        \n",
        "        gen.train() # Retour en mode train"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
