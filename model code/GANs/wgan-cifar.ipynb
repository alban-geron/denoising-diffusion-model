{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf41123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "343079d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement sur : cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Hyperparamètres WGAN-GP Avancé\n",
    "    LR = 1e-4\n",
    "    BATCH_SIZE = 64\n",
    "    IMAGE_SIZE = 32\n",
    "    CHANNELS = 3\n",
    "    Z_DIM = 100\n",
    "    NUM_EPOCHS = 100        # CIFAR demande du temps\n",
    "    FEATURES_DIM = 128      # Augmenté pour la capacité (Symétrie G/C)\n",
    "    CRITIC_ITERATIONS = 5\n",
    "    LAMBDA_GP = 10\n",
    "    EMA_DECAY = 0.999       # Lissage des poids\n",
    "    \n",
    "    # Chemins\n",
    "    DATA_ROOT = os.path.join(\"..\", \"..\", \"dataset\", \"CIFAR\")\n",
    "    IMG_DIR = \"samples-cifar\"\n",
    "    CKPT_NAME = \"wgan_cifar_advanced_ckpt.pth\"\n",
    "    SAVE_EVERY = 5\n",
    "\n",
    "conf = Config()\n",
    "print(f\"Entraînement sur : {conf.DEVICE}\")\n",
    "\n",
    "# Création des dossiers\n",
    "os.makedirs(conf.IMG_DIR, exist_ok=True)\n",
    "os.makedirs(conf.DATA_ROOT, exist_ok=True)\n",
    "ckpt_path_full = conf.CKPT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9e387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. MODULES UTILITAIRES (EMA & ATTENTION)\n",
    "# ==========================================\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average pour stabiliser le générateur.\"\"\"\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data\n",
    "                self.shadow[name] = self.shadow[name].to(conf.DEVICE)\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]\n",
    "    \n",
    "    # Nécessaire pour sauvegarder l'état de l'EMA dans le .pth\n",
    "    def state_dict(self):\n",
    "        return self.shadow\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.shadow = state_dict\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Bloc d'attention pour capturer les dépendances globales.\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "        return self.gamma * out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec187cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. ARCHITECTURES AVANCÉES\n",
    "# ==========================================\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Critic, self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            # 32x32 -> 16x16\n",
    "            nn.Conv2d(channels_img, features_d, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # 16x16 -> 8x8\n",
    "            nn.Conv2d(features_d, features_d * 2, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(features_d * 2, affine=True), # InstanceNorm\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # Attention à la résolution 8x8\n",
    "        self.attn = SelfAttention(features_d * 2)\n",
    "        \n",
    "        self.final_layers = nn.Sequential(\n",
    "            # 8x8 -> 4x4\n",
    "            nn.Conv2d(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(features_d * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 4x4 -> 1x1\n",
    "            nn.Conv2d(features_d * 4, 1, 4, 1, 0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.critic(x)\n",
    "        out = self.attn(out)\n",
    "        return self.final_layers(out)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Bloc helper : Upsample + Conv (Anti-Checkerboard)\n",
    "        def block(in_channels, out_channels, normalize=True):\n",
    "            layers = [\n",
    "                nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            ]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU())\n",
    "            return layers\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            # Z -> 4x4\n",
    "            nn.ConvTranspose2d(z_dim, features_g * 4, 4, 1, 0),\n",
    "            nn.BatchNorm2d(features_g * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # 4x4 -> 8x8\n",
    "            *block(features_g * 4, features_g * 2),\n",
    "        )\n",
    "        \n",
    "        # Attention à la résolution 8x8\n",
    "        self.attn = SelfAttention(features_g * 2)\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            # 8x8 -> 16x16\n",
    "            *block(features_g * 2, features_g),\n",
    "            # 16x16 -> 32x32\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(features_g, channels_img, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.initial(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.attn(out)\n",
    "        return self.layer2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0729d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. GRADIENT PENALTY\n",
    "# ==========================================\n",
    "def gradient_penalty(critic, real, fake, device):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    epsilon = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated = real * epsilon + fake * (1 - epsilon)\n",
    "    interpolated.requires_grad_(True)\n",
    "    mixed_scores = critic(interpolated)\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    return torch.mean((gradient_norm - 1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acca3140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ..\\..\\dataset\\CIFAR\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:18<00:00, 9.36MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ..\\..\\dataset\\CIFAR\\cifar-10-python.tar.gz to ..\\..\\dataset\\CIFAR\n",
      "Démarrage de zéro.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. INITIALISATION\n",
    "# ==========================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(conf.IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root=conf.DATA_ROOT, train=True, transform=transform, download=True)\n",
    "loader = DataLoader(dataset, batch_size=conf.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Instanciation avec FEATURES_DIM = 128\n",
    "gen = Generator(conf.Z_DIM, conf.CHANNELS, conf.FEATURES_DIM).to(conf.DEVICE)\n",
    "critic = Critic(conf.CHANNELS, conf.FEATURES_DIM).to(conf.DEVICE)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=conf.LR, betas=(0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=conf.LR, betas=(0.0, 0.9))\n",
    "\n",
    "# Initialisation EMA\n",
    "ema = EMA(gen, decay=conf.EMA_DECAY)\n",
    "ema.register()\n",
    "\n",
    "fixed_noise = torch.randn(64, conf.Z_DIM, 1, 1).to(conf.DEVICE)\n",
    "\n",
    "# Reprise de Checkpoint\n",
    "start_epoch = 0\n",
    "if os.path.exists(ckpt_path_full):\n",
    "    print(f\"Chargement du checkpoint : {ckpt_path_full}\")\n",
    "    ckpt = torch.load(ckpt_path_full, map_location=conf.DEVICE)\n",
    "    gen.load_state_dict(ckpt[\"gen_state\"])\n",
    "    critic.load_state_dict(ckpt[\"critic_state\"])\n",
    "    opt_gen.load_state_dict(ckpt[\"opt_gen_state\"])\n",
    "    opt_critic.load_state_dict(ckpt[\"opt_critic_state\"])\n",
    "    # Chargement état EMA\n",
    "    if \"ema_state\" in ckpt:\n",
    "        ema.load_state_dict(ckpt[\"ema_state\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "else:\n",
    "    print(\"Démarrage de zéro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3fec33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de l'entraînement CIFAR-10 Avancé...\n",
      "--> Epoch 5\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00377 sec\n",
      "    [TIMING] Temps moyen par image : 0.000059 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_5.png\n",
      "--> Epoch 10\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00218 sec\n",
      "    [TIMING] Temps moyen par image : 0.000034 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_10.png\n",
      "--> Epoch 15\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00103 sec\n",
      "    [TIMING] Temps moyen par image : 0.000016 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_15.png\n",
      "--> Epoch 20\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00299 sec\n",
      "    [TIMING] Temps moyen par image : 0.000047 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_20.png\n",
      "--> Epoch 25\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00269 sec\n",
      "    [TIMING] Temps moyen par image : 0.000042 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_25.png\n",
      "--> Epoch 30\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00331 sec\n",
      "    [TIMING] Temps moyen par image : 0.000052 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_30.png\n",
      "--> Epoch 35\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00374 sec\n",
      "    [TIMING] Temps moyen par image : 0.000058 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_35.png\n",
      "--> Epoch 40\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00157 sec\n",
      "    [TIMING] Temps moyen par image : 0.000024 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_40.png\n",
      "--> Epoch 45\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00668 sec\n",
      "    [TIMING] Temps moyen par image : 0.000104 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_45.png\n",
      "--> Epoch 50\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00300 sec\n",
      "    [TIMING] Temps moyen par image : 0.000047 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_50.png\n",
      "--> Epoch 55\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00573 sec\n",
      "    [TIMING] Temps moyen par image : 0.000090 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_55.png\n",
      "--> Epoch 60\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00252 sec\n",
      "    [TIMING] Temps moyen par image : 0.000039 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_60.png\n",
      "--> Epoch 65\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00000 sec\n",
      "    [TIMING] Temps moyen par image : 0.000000 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_65.png\n",
      "--> Epoch 70\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00201 sec\n",
      "    [TIMING] Temps moyen par image : 0.000031 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_70.png\n",
      "--> Epoch 75\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00451 sec\n",
      "    [TIMING] Temps moyen par image : 0.000071 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_75.png\n",
      "--> Epoch 80\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00418 sec\n",
      "    [TIMING] Temps moyen par image : 0.000065 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_80.png\n",
      "--> Epoch 85\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00200 sec\n",
      "    [TIMING] Temps moyen par image : 0.000031 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_85.png\n",
      "--> Epoch 90\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00251 sec\n",
      "    [TIMING] Temps moyen par image : 0.000039 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_90.png\n",
      "--> Epoch 95\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00323 sec\n",
      "    [TIMING] Temps moyen par image : 0.000051 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_95.png\n",
      "--> Epoch 100\n",
      "    [TIMING] Temps de Forward (EMA) pour 64 images : 0.00296 sec\n",
      "    [TIMING] Temps moyen par image : 0.000046 sec\n",
      "    Image sauvegardée : samples-cifar\\epoch_100.png\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. BOUCLE D'ENTRAÎNEMENT + TIMING\n",
    "# ==========================================\n",
    "print(\"Début de l'entraînement CIFAR-10 Avancé...\")\n",
    "\n",
    "for epoch in range(start_epoch, conf.NUM_EPOCHS):\n",
    "    gen.train()\n",
    "    critic.train()\n",
    "    \n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.to(conf.DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        # --- Train Critic ---\n",
    "        for _ in range(conf.CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(cur_batch_size, conf.Z_DIM, 1, 1).to(conf.DEVICE)\n",
    "            fake = gen(noise)\n",
    "            critic_real = critic(real).reshape(-1)\n",
    "            critic_fake = critic(fake).reshape(-1)\n",
    "            gp = gradient_penalty(critic, real, fake, conf.DEVICE)\n",
    "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake)) + conf.LAMBDA_GP * gp\n",
    "            \n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "\n",
    "        # --- Train Generator ---\n",
    "        gen_fake = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        \n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        # Mise à jour EMA\n",
    "        ema.update()\n",
    "\n",
    "    # ==========================================\n",
    "    # SAUVEGARDE, GÉNÉRATION ET CHRONOMÉTRAGE\n",
    "    # ==========================================\n",
    "    if (epoch + 1) % conf.SAVE_EVERY == 0 or (epoch + 1) == conf.NUM_EPOCHS:\n",
    "        print(f\"--> Epoch {epoch+1}\")\n",
    "        \n",
    "        # 1. Sauvegarde PTH (incluant EMA)\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"gen_state\": gen.state_dict(),\n",
    "            \"critic_state\": critic.state_dict(),\n",
    "            \"opt_gen_state\": opt_gen.state_dict(),\n",
    "            \"opt_critic_state\": opt_critic.state_dict(),\n",
    "            \"ema_state\": ema.state_dict(), # On sauvegarde les poids lissés\n",
    "        }, ckpt_path_full)\n",
    "        \n",
    "        # 2. Utilisation de l'EMA pour la génération et le timing\n",
    "        ema.apply_shadow() # On charge les poids lissés dans le modèle\n",
    "        gen.eval()\n",
    "        \n",
    "        # --- CHRONOMÈTRE (FORWARD) ---\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake_img = gen(fixed_noise) # FORWARD PASS\n",
    "            \n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        forward_duration = end_time - start_time\n",
    "        \n",
    "        print(f\"    [TIMING] Temps de Forward (EMA) pour {conf.BATCH_SIZE} images : {forward_duration:.5f} sec\")\n",
    "        print(f\"    [TIMING] Temps moyen par image : {forward_duration/conf.BATCH_SIZE:.6f} sec\")\n",
    "\n",
    "        # 3. Sauvegarde Image\n",
    "        fake_img = (fake_img * 0.5) + 0.5\n",
    "        save_path = os.path.join(conf.IMG_DIR, f\"epoch_{epoch+1}.png\")\n",
    "        save_image(fake_img, save_path, nrow=8)\n",
    "        print(f\"    Image sauvegardée : {save_path}\")\n",
    "        \n",
    "        # 4. Restauration\n",
    "        ema.restore() # On remet les poids normaux pour continuer l'entraînement\n",
    "        gen.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
